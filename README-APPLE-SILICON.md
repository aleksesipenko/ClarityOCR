# ClarityOCR - Apple Silicon Support

ClarityOCR now supports Apple Silicon (M1, M2, M3 Pro/Max/Ultra) with GPU acceleration via **Metal Performance Shaders (MPS)**.

## üçé Supported Devices

- **M1 / M1 Pro / M1 Max / M1 Ultra**
- **M2 / M2 Pro / M2 Max / M2 Ultra**
- **M3 / M3 Pro / M3 Max / M3 Ultra**

## ‚ö° Installation (Apple Silicon)

### Option 1: Using PyTorch with MPS (Recommended)

```bash
# Clone repository
git clone https://github.com/aleksesipenko/ClarityOCR.git
cd ClarityOCR

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies for Apple Silicon
pip install -r requirements-apple-silicon.txt
```

### Option 2: Manual Installation

```bash
# Install PyTorch with MPS support
pip install torch>=2.0.0

# Install other dependencies
pip install \
    marker-pdf>=0.3.0 \
    fastapi>=0.100.0 \
    uvicorn>=0.23.0 \
    openai>=1.0.0 \
    pypdfium2>=4.0.0
```

## üîç Device Detection

ClarityOCR automatically detects the best available device:

1. **CUDA** (NVIDIA GPUs) - highest priority
2. **MPS** (Apple Silicon) - medium priority
3. **CPU** - fallback

To check which device is detected:

```bash
python -m clarityocr.device_utils
```

Example output:
```
=== ClarityOCR Device Detection ===
Device Type: mps
Device Name: MPS (Apple Silicon: arm)
GPU Available: True

Configuration:
‚úÖ MPS configured (Apple Silicon GPU)

Memory Info:
  Total: 16.00 GB
  Used: 8.50 GB
  Free: 7.50 GB
```

## üöÄ Running on Apple Silicon

### Web UI

```bash
# Start web server
python -m clarityocr.server

# Or using uvicorn directly
uvicorn clarityocr.server:app --host 0.0.0.0 --port 8000
```

### CLI

```bash
# Convert PDF to Markdown
python -m clarityocr.converter input.pdf output.md

# With specific settings
python -m clarityocr.converter input.pdf output.md --layout-batch 4 --recognition-batch 8
```

## ‚öôÔ∏è Configuration

### MPS-Specific Settings

For optimal performance on Apple Silicon, consider these settings:

```bash
# Smaller batch sizes work better on MPS
python -m clarityocr.converter input.pdf output.md \
    --layout-batch 2 \
    --recognition-batch 4
```

### Mixed Precision

MPS supports bfloat16 operations for faster inference:

```python
import torch
torch.backends.mps.allow_tf32 = True  # Enabled by default in ClarityOCR
```

## üìä Performance

Expected performance on different Apple Silicon chips:

| Chip | Unified Memory | Relative Speed* |
|------|---------------|-----------------|
| M1   | 8-16 GB       | 1x              |
| M1 Pro/Max | 16-32 GB | 1.5-2x          |
| M2   | 8-24 GB       | 1.2-1.5x        |
| M2 Pro/Max | 16-96 GB | 2-3x            |
| M3   | 8-24 GB       | 1.5-2x          |
| M3 Pro/Max | 16-128 GB | 2.5-3.5x         |

\*Compared to CPU baseline

## üîß Troubleshooting

### MPS not detected

If MPS is not detected:

```bash
# Check PyTorch version
python -c "import torch; print(torch.__version__)"

# Check MPS availability
python -c "import torch; print(torch.backends.mps.is_available())"
```

If MPS is not available:
1. Ensure you're running on Apple Silicon (not Intel Mac)
2. Update PyTorch: `pip install --upgrade torch`
3. Check macOS version (macOS 12.3+ required for MPS)

### Out of Memory Errors

Apple Silicon uses **unified memory** (shared CPU/GPU). If you encounter OOM:

```bash
# Reduce batch sizes
python -m clarityocr.converter input.pdf output.md \
    --layout-batch 1 \
    --recognition-batch 2

# Close other memory-intensive applications
# Activity Monitor can help identify memory usage
```

### Slow Performance

If conversion is slow:

1. Check Activity Monitor for GPU utilization
2. Reduce batch sizes if GPU is underutilized
3. Ensure no other heavy GPU workloads are running

## üÜö CUDA vs MPS

| Feature | CUDA (NVIDIA) | MPS (Apple Silicon) |
|---------|---------------|---------------------|
| VRAM | Dedicated GPU memory | Unified memory |
| Precision | BF16/FP16 | BF16 |
| Batch sizes | Larger (8-32) | Smaller (2-8) |
| nvidia-smi | Available | Not applicable |
| Memory monitoring | Detailed | Basic |

## üìù Notes

- **Unified Memory**: Apple Silicon shares memory between CPU and GPU. Close other applications before large conversions.
- **No nvidia-smi**: MPS doesn't have an equivalent to nvidia-smi. Use Activity Monitor instead.
- **Batch sizes**: MPS generally prefers smaller batch sizes than CUDA.
- **First run**: The first conversion may be slower as models load into memory.

## üêõ Reporting Issues

When reporting issues on Apple Silicon, include:

1. Mac model and chip (e.g., MacBook Pro M2)
2. macOS version
3. PyTorch version (`python -c "import torch; print(torch.__version__)"`)
4. Device detection output (`python -m clarityocr.device_utils`)
5. Error messages or logs

## üìö Resources

- [PyTorch MPS Documentation](https://pytorch.org/docs/stable/notes/mps.html)
- [Apple Developer - Metal](https://developer.apple.com/metal/)
- [ClarityOCR GitHub](https://github.com/aleksesipenko/ClarityOCR)

---

**Happy OCR on Apple Silicon! üçé**
